{"name":"Apache Software Foundation","description":"Open source software to the public free of charge","gsoc_url":"https://summerofcode.withgoogle.com/programs/2025/organizations/apache-software-foundation","ideas_url":"https://s.apache.org/gsoc2025ideas","logo":"https://summerofcode.withgoogle.com/media/org/apache-software-foundation/hq22fwtmvdfjnsh9-360.png","technologies":["c","java","c++"],"topics":["big data","cloud","libraries","other"],"projects":[{"project_name":"Migrate Apache Airavata Deployment from Ansible to OpenTofu","summary":"Replace existing Ansible deployment scripts with OpenTofu configurations for improved efficiency and maintainability in Airavata deployment processes.","difficulty":"Major"},{"project_name":"Develop an Integrated Feature Test Environment for Apache Airavata","summary":"Implement a simulated High-Performance Computing (HPC) environment for testing in Airavata's IDE, enabling feature validation without physical HPC resources.","difficulty":"Major"},{"project_name":"A Central Admin Dashboard to Inspect Health + Logs of Airavata Services","summary":"Create a dashboard for real-time monitoring of Airavata services, including health checks and log aggregation from multiple gateways.","difficulty":"Major"},{"project_name":"Streamline Grouping and Filtering in the Experiment Browser UI","summary":"Enhance the user interface for grouping and displaying past experiments in the Airavata Django portal for better insights and tracking.","difficulty":"Major"},{"project_name":"Update Airavata Django Portal to a Supported Python Version","summary":"Upgrade the Airavata Django Portal to Python 3.12+ to improve security and access to new features, ensuring long-term compatibility.","difficulty":"Major"},{"project_name":"Containerized Deployment of Airavata Services","summary":"Containerize all Airavata services using Docker to enable consistent deployments and ease of management across different environments.","difficulty":"Major"},{"project_name":"GSoC 2025 - Service Discovery for Apache Dubbo","summary":"Develop enhanced service discovery solutions, improving logging, actuator endpoints, and tools for a more efficient microservice architecture.","difficulty":"Major"},{"project_name":"GSoC 2025 - Add More Traffic Management Rule Support for Dubbo Proxyless Mesh","summary":"Implement additional traffic management features for Dubbo to enhance its compatibility with service mesh architectures.","difficulty":"Major"},{"project_name":"GSoC 2025 - Dubbo Admin Traffic Management Feature","summary":"Refine Dubbo Admin to ensure it generates and handles traffic management rules effectively for microservice applications.","difficulty":"Major"},{"project_name":"GSoC 2025 - Enhancing Dubbo Python Serialization","summary":"Build an integrated serialization layer in Dubbo Python to support common formats and ease the serialization process for users.","difficulty":"Major"},{"project_name":"GSoC 2025 - Dubbo Triple Protocol for Go Language Implementation","summary":"Implement the Dubbo triple protocol in Go language to improve performance and connection management for microservice interactions.","difficulty":"Major"},{"project_name":"GSoC 2025 - Dubbo Gradle IDL Plugin","summary":"Develop a Gradle plugin for Dubbo to enable seamless generation of IDL files, improving the developer experience and integration with IDEs.","difficulty":"Major"},{"project_name":"GSoC 2025 - Apache Seata Extend Multi-Raft Cluster Mode","summary":"Enhance Apache Seata server performance by implementing support for multi-Raft clusters to distribute load more effectively.","difficulty":"Major"},{"project_name":"GSoC 2025 - Apache Seata Unlocking Metadata for Load Balancing and Routing","summary":"Add metadata support to Seata's registry for more sophisticated load balancing strategies based on custom server metadata.","difficulty":"Major"},{"project_name":"GSoC 2025 - Improve the Kvrocks Controller UI","summary":"Refactor and modernize the user interface of the Kvrocks Controller for better usability and visualization of cluster activities.","difficulty":"Major"},{"project_name":"GSoC 2025 - Support Database Backup to Cloud Storage in Kvrocks","summary":"Implement a backup mechanism for Kvrocks that allows users to store backups on cloud storage platforms like AWS, Google Cloud, and Azure.","difficulty":"Major"},{"project_name":"Enhancing Apache Beam JupyterLab Sidepanel for JupyterLab 4.x and Improved UI/UX","summary":"Upgrade the Apache Beam sidepanel for JupyterLab to ensure compatibility with version 4.x and improve user experience and interactivity.","difficulty":"Major"},{"project_name":"Beam YAML ML, Iceberg, and Kafka User Accessibility","summary":"Enhance documentation and examples for Beam's YAML DSL, focusing on machine learning workflows and integration with Iceberg and Kafka.","difficulty":"Major"},{"project_name":"Beam ML Vector DB/Feature Store Integrations","summary":"Build connectors to ML feature stores and vector databases from Apache Beam, facilitating use cases in machine learning.","difficulty":"Major"},{"project_name":"Enhance Lineage Support in Beam","summary":"Expand support for data lineage in Apache Beam to help users track data movement and transformations in their pipelines.","difficulty":"Major"},{"project_name":"Optimizing Apache RocketMQ's POP Orderly Consumption Process","summary":"Refactor and optimize the POP Orderly consumption process in RocketMQ for improved reliability and performance in distributing messages.","difficulty":"Major"},{"project_name":"Refactoring the RocketMQ Dashboard UI and Enhancing Usability","summary":"Redesign the RocketMQ Dashboard UI to enhance user experience and improve the security of data interactions.","difficulty":"Major"},{"project_name":"SkyWalking BanyanDB Extend remote.FS with Object Storage Support for AWS, Google Cloud, and Azure","summary":"Integrate support for popular object storage services into SkyWalking's remote file storage interface, enabling more robust backup solutions.","difficulty":"Major"},{"project_name":"Implement Agentic GraphRAG Architecture in HugeGraph","summary":"Introduce an Agentic architecture in HugeGraph to improve flexibility and adaptability for complex graph operations using dynamic processing techniques.","difficulty":"Major"},{"project_name":"Enhancing Apache DolphinScheduler with Generalized OIDC Authentication","summary":"Implement a flexible and generalized OpenID Connect authentication mechanism for better integration with enterprise user accounts in DolphinScheduler.","difficulty":"Major"},{"project_name":"Apache Lucene.NET Replicator and Dependency Injection Enhancements","summary":"Enhance the Lucene.NET replication capabilities and streamline dependencies for a smoother user experience in distributed systems.","difficulty":"Major"},{"project_name":"Apache CloudStack DRS Improvements","summary":"Improve the distribution of resources in Apache CloudStack by refining the Distributed Resource Scheduler for better load balancing across zones.","difficulty":"Major"},{"project_name":"Verification of LDAP Connection in CloudStack","summary":"Implement diagnostics to validate new LDAP connections in CloudStack, simplifying troubleshooting and configuration management.","difficulty":"Major"},{"project_name":"SSL - LetsEncrypt the Console Proxy","summary":"Integrate LetsEncrypt SSL certificates into CloudStack's console proxy for enhanced security in web-based management interfaces.","difficulty":"Major"},{"project_name":"StreamPipes Visualization Enhancements","summary":"Extend the visualization capabilities in Apache StreamPipes by adding new types of charts and advanced configurations for better data presentation.","difficulty":"Major"},{"project_name":"HertzBeat AI Agent Based on the MCP Protocol","summary":"Develop an AI-based agent in HertzBeat for conversational interactions to query monitoring information and configure tasks.","difficulty":"Major"},{"project_name":"Apache Mahout Refactoring the Website","summary":"Refactor the Apache Mahout website to focus on Quantum Computing initiatives and improve overall usability and navigation for users.","difficulty":"Major"}],"jina_response":"Title: GSoC 2025 Ideas list - Community Development\n\nURL Source: https://s.apache.org/gsoc2025ideas\n\nMarkdown Content:\nThis page is auto-generated! Please do NOT edit it, all changes will be lost on next update\n\nContents\n\n*   [Airavata](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-Airavata)\n\n*   [Migrate Apache Airavata Deployment from Ansible to OpenTofu](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-MigrateApacheAiravataDeploymentfromAnsibletoOpenTofu)\n\n*   [Develop an Integrated Feature Test Environment for Apache Airavata](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-DevelopanIntegratedFeatureTestEnvironmentforApacheAiravata)\n\n*   [A Central Admin Dashboard to Inspect Health + Logs of Airavata Services](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-ACentralAdminDashboardtoInspectHealth+LogsofAiravataServices)\n\n*   [Streamline Grouping and Filtering in the Experiment Browser UI](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-StreamlineGroupingandFilteringintheExperimentBrowserUI)\n\n*   [Update Airavata Django Portal to a Supported Python Version](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-UpdateAiravataDjangoPortaltoaSupportedPythonVersion)\n\n*   [Containerized Deployment of Airavata Services](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-ContainerizedDeploymentofAiravataServices)\n\n*   [Apache Dubbo](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-ApacheDubbo)\n\n*   [GSoC 2025 - Service Discovery](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-GSoC2025-ServiceDiscovery)\n\n*   [GSoC 2025 - Add more traffic management rule support for Dubbo Proxyless Mesh](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-GSoC2025-AddmoretrafficmanagementrulesupportforDubboProxylessMesh)\n\n*   [GSoC 2025 - Dubbo Admin traffic management feature](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-GSoC2025-DubboAdmintrafficmanagementfeature)\n\n*   [GSoC 2025 - Enhancing Dubbo Python Serialization](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-GSoC2025-EnhancingDubboPythonSerialization)\n\n*   [GSoC 2025 - Dubbo triple protocol for go language implementation](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-GSoC2025-Dubbotripleprotocolforgolanguageimplementation)\n\n*   [GSoC 2025 - Dubbo Gradle IDL Plugin](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-GSoC2025-DubboGradleIDLPlugin)\n\n*   [Seata](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-Seata)\n\n*   [GSoC 2025 - Apache Seata(Incubating) Extend multi-raft cluster mode](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-GSoC2025-ApacheSeata(Incubating)Extendmulti-raftclustermode)\n\n*   [GSoC 2025 - Apache Seata(Incubating)Unlocking the Power of Metadata in Apache Seata From Load Balancing to Advanced Routing](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-GSoC2025-ApacheSeata(Incubating)UnlockingthePowerofMetadatainApacheSeataFromLoadBalancingtoAdvancedRouting)\n\n*   [Kvrocks](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-Kvrocks)\n\n*   [\\[GSOC\\]\\[Kvrocks\\] Improve the controller UI](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-[GSOC][Kvrocks]ImprovethecontrollerUI)\n\n*   [\\[GSOC\\]\\[Kvrocks\\] Support database backup to cloud storage](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-[GSOC][Kvrocks]Supportdatabasebackuptocloudstorage)\n\n*   [Beam](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-Beam)\n\n*   [Simplify management of Beam infrastructure, access control and permissions via Platform features](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-SimplifymanagementofBeaminfrastructure,accesscontrolandpermissionsviaPlatformfeatures)\n\n*   [Enhancing Apache Beam JupyterLab Sidepanel for JupyterLab 4.x and Improved UI/UX](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-EnhancingApacheBeamJupyterLabSidepanelforJupyterLab4.xandImprovedUI/UX)\n\n*   [Beam YAML ML, Iceberg, and Kafka User Accessibility](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-BeamYAMLML,Iceberg,andKafkaUserAccessibility)\n\n*   [Beam ML Vector DB/Feature Store integrations](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-BeamMLVectorDB/FeatureStoreintegrations)\n\n*   [Enhance lineage support in Beam](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-EnhancelineagesupportinBeam)\n\n*   [RocketMQ](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-RocketMQ)\n\n*   Optimizing Apache RocketMQ's POP Orderly Consumption Process\n\n*   [Refactoring the RocketMQ Dashboard UI and Enhancing Usability](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-RefactoringtheRocketMQDashboardUIandEnhancingUsability)\n\n*   [SkyWalking](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-SkyWalking)\n\n*   [SkyWalking BanyanDB Extend remote.FS with Object Storage Support for AWS, Google Cloud, and Azure](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-SkyWalkingBanyanDBExtendremote.FSwithObjectStorageSupportforAWS,GoogleCloud,andAzure)\n\n*   [HugeGraph](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-HugeGraph)\n\n*   [\\[GSoC\\]\\[HugeGraph\\] Implement Agentic GraphRAG Architecture](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-[GSoC][HugeGraph]ImplementAgenticGraphRAGArchitecture)\n\n*   [DolphinScheduler](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-DolphinScheduler)\n\n*   [Enhancing Apache DolphinScheduler with Generalized OIDC Authentication](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-EnhancingApacheDolphinSchedulerwithGeneralizedOIDCAuthentication)\n\n*   [Lucene.NET](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-Lucene.NET)\n\n*   [Apache Lucene.NET Replicator and Dependency Injection Enhancements](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-ApacheLucene.NETReplicatorandDependencyInjectionEnhancements)\n\n*   [CloudStack](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-CloudStack)\n\n*   [Apache CloudStack DRS improvements](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-ApacheCloudStackDRSimprovements)\n\n*   [verification of LDAP connection](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-verificationofLDAPconnection)\n\n*   [SSL - LetsEncrypt the Console Proxy](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-SSL-LetsEncrypttheConsoleProxy)\n\n*   [Autodetect IPs used inside the VM on L2 networks](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-AutodetectIPsusedinsidetheVMonL2networks)\n\n*   [eBPF-based Network Observability for CloudStack](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-eBPF-basedNetworkObservabilityforCloudStack)\n\n*   [Enhancing CloudStack Monitoring with eBPF](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-EnhancingCloudStackMonitoringwitheBPF)\n\n*   [\\[GSoC\\] \\[CloudStack\\] Improve CloudMonkey user experience by enhancing autocompletion](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-[GSoC][CloudStack]ImproveCloudMonkeyuserexperiencebyenhancingautocompletion)\n\n*   [add securitata integration to cloudstack](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-addsecuritataintegrationtocloudstack)\n\n*   [StreamPipes](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-StreamPipes)\n\n*   [Extend visualization capabilities of Apache StreamPipes](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-ExtendvisualizationcapabilitiesofApacheStreamPipes)\n\n*   [HertzBeat](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-HertzBeat)\n\n*   [\\[GSOC\\]\\[HertzBeat\\] AI Agent Based on the MCP Protocol for Monitoring Info Interaction](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-[GSOC][HertzBeat]AIAgentBasedontheMCPProtocolforMonitoringInfoInteraction)\n\n*   [Comdev GSOC](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-ComdevGSOC)\n\n*   [Apache Mahout Refactoring the Website](https://s.apache.org/gsoc2025ideas#GSoC2025Ideaslist-ApacheMahoutRefactoringtheWebsite)\n\nAiravata\n--------\n\n### [Migrate Apache Airavata Deployment from Ansible to OpenTofu](https://issues.apache.org/jira/browse/AIRAVATA-3955)\n\nObjective\n---------\n\nReplace existing Ansible deployment scripts with OpenTofu configurations to improve deployment efficiency and maintainability for bare-metal environments.\n\nRequirements\n------------\n\n*   Assessment of Current Ansible Scripts\n    *   **Review Existing Playbooks**: Analyze the current Ansible playbooks located in the [Airavata GitHub repository](https://github.com/apache/airavata/blob/master/dev-tools/ansible/README.md) to understand the deployment processes and dependencies.\n    *   **Identify Core Components**: Determine the essential services and configurations managed by Ansible, such as Kafka, RabbitMQ, Zookeeper, MariaDB, etc.\n\n*   Development of OpenTofu Configurations\n    *   **Define Infrastructure as Code (IaC)**: Utilize OpenTofu's declarative language to codify the infrastructure components identified in the assessment phase.\n    *   **Module Creation**: Develop reusable modules for each service (e.g., Kafka, RabbitMQ, Zookeeper) to promote consistency and ease of management.\n\n*   Testing and Validation\n    *   **Simulate Deployments**: Use OpenTofu's planning capabilities to simulate deployments, ensuring configurations align with the desired infrastructure state.\n    *   **Iterative Refinement**: Address any discrepancies or issues identified during testing to refine the OpenTofu configurations.\n\n*   Documentation\n    *   **Update Deployment Guides**: Revise existing documentation to reflect the new OpenTofu-based deployment process, providing clear instructions for users.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Lahiru Jayathilake_, mail: lahirujayathilake (at) apache.org\n\n_Project Devs_, mail: dev (at) airavata.apache.org\n\n### [Develop an Integrated Feature Test Environment for Apache Airavata](https://issues.apache.org/jira/browse/AIRAVATA-3954)\n\nObjective\n---------\n\nEnhance the current development workflow by incorporating a simulated High-Performance Computing (HPC) environment into Apache Airavata's existing [Integrated Development Environment (IDE) integration](https://github.com/apache/airavata/blob/master/modules/ide-integration/README.md). This will enable developers to test and validate features locally without relying on physical HPC resources.\n\nRequirements\n------------\n\n*   Simulated HPC Environment Integration\n    *   **Dockerized Slurm Simulation**: Develop a Docker container that emulates an HPC environment using Slurm, facilitating the testing of job scheduling and execution.​\n    *   **Seamless IDE Integration**: Ensure that this simulated environment integrates smoothly with the existing IDE setup, allowing developers to initiate and monitor jobs as they would in a real HPC setting.\n\n*   Development of Comprehensive Test Scenarios\n    *   **Job Submission Tests**: Create scripts to test various job submission scenarios, including successful executions, intentional failures, and long-running processes.​\n    *   **Feature Validation**: Ensure that all features exposed by Apache Airavata can be tested within this simulated environment.\n\n*   User-Friendly Setup\n    *   **Simplified Configuration**: Design the setup process to require minimal configuration, enabling developers to initiate the environment and execute tests with just a few commands\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Lahiru Jayathilake_, mail: lahirujayathilake (at) apache.org\n\n_Project Devs_, mail: dev (at) airavata.apache.org\n\n### [A Central Admin Dashboard to Inspect Health + Logs of Airavata Services](https://issues.apache.org/jira/browse/AIRAVATA-3958)\n\nDevelop a devops dashboard to monitor Apache Airavata services, enabling real-time tracking of service health, uptime, and logs independent of the science gateway(s).  \nThis centralized tool will help administrators efficiently monitor service performance and troubleshoot issues. The dashboard will feature a user-friendly monitoring UI that displays real-time status updates and logs for each service.\n\nProposed Solution:\n\n*   A logging subproces alongside each service, pushing logs to an external service.\n*   A devops dashboard that aggregates the logs and provides a unified view into the system.\n*   API calls from devops dashboard to each service, for proactive health-checking.\n*   Ability to monitor multiple gateways from the same dashboard.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Yasith Jayawardana_, mail: yasithmilinda (at) apache.org\n\n_Project Devs_, mail: dev (at) airavata.apache.org\n\n### [Streamline Grouping and Filtering in the Experiment Browser UI](https://issues.apache.org/jira/browse/AIRAVATA-3957)\n\nEmbed a researcher-focused dashboard to group and preview experiments in the Django portal. The goal is to improve how past experiment runs can be tracked, grouped, and arranged for faster lookup and insights.\n\nProposed Improvements:\n\n*   Group submitted experiments by project, application, allocation, etc.\n*   Clean, customizable dashboard elements (e.g., charts) to preview past experiments.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Yasith Jayawardana_, mail: yasithmilinda (at) apache.org\n\n_Project Devs_, mail: dev (at) airavata.apache.org\n\n### [Update Airavata Django Portal to a Supported Python Version](https://issues.apache.org/jira/browse/AIRAVATA-3956)\n\nThe Airavata Django Portal currently runs on Python 3.6, which reached its end-of-life (EOL) in 2022. Continuing to use an unsupported Python version poses security risks and limits access to new features and package updates. Upgrading to a supported version (Python 3.12 or later) will ensure long-term maintainability, security, and compatibility with modern dependencies.\n\n[Status of Python versions](https://devguide.python.org/versions/)\n\nImpact:  \n• Improved security and stability  \n• Access to the latest language features and performance improvements  \n• Compatibility with actively maintained third-party packages\n\nProposed Solution: Update the codebase and dependencies for compatibility with Python 3.12+, and ensure everything works as expected post-upgrade.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Yasith Jayawardana_, mail: yasithmilinda (at) apache.org\n\n_Project Devs_, mail: dev (at) airavata.apache.org\n\n### [Containerized Deployment of Airavata Services](https://issues.apache.org/jira/browse/AIRAVATA-3959)\n\nCurrently, all Airavata services are packaged and deployed as Java bundles. The goal is to containerize each service by wrapping it within a Dockerfile, allowing seamless deployment on container-enabled resources while also enabling local execution for development purposes.\n\nThis enhancement has potential to improve deployment consistency, simplify dependency management, and provide greater flexibility in running Airavata services across different environments, for both testing and production use cases.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Yasith Jayawardana_, mail: yasithmilinda (at) apache.org\n\n_Project Devs_, mail: dev (at) airavata.apache.org\n\nApache Dubbo\n------------\n\n### [GSoC 2025 - Service Discovery](https://issues.apache.org/jira/browse/DUBBO-143)\n\nBackground and Goal\n-------------------\n\nService Discovery\n\n1.  Well organized logs\n2.  Actuator endpoints\n3.  Tools\n\nRelevant Skills\n---------------\n\n1.  Familiar with Java\n2.  Familiar with Microservice architecture\n\nPotential Mentors\n-----------------\n\n1.  Jun Liu, Apache Dubbo PMC Chair, [junliu@apache.org![Image 1](https://issues.apache.org/jira/images/icons/mail_small.gif)](mailto:junliu@apache.org)\n2.  dev@dubbo.apache.org\n    \n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Jun Liu_, mail: liujun (at) apache.org\n\n_Project Devs_, mail:\n\n### [GSoC 2025 - Add more traffic management rule support for Dubbo Proxyless Mesh](https://issues.apache.org/jira/browse/DUBBO-141)\n\nBackground and Goal\n-------------------\n\nThe concept of\\[ Proxyless Mesh|https://istio.io/v1.15/blog/2021/proxyless-grpc/\\] was first introduced in this blog. Please read it to learn more concept details.\n\nWe have started the development of Dubbo Proxyss Mesh for a while, so that means you don't have to start the project from scratch, anyone who gets involved can start with a specific task at hand.\n\nIn this specific GSoC project, we need developers to mainly focus on implementing more traffic management features of Istio for Dubbo.\n\nRelevant Skills\n---------------\n\n1.  Familiar with Java\n2.  Familiar with Service Mesh, istio and Microservice architectures\n3.  Familiar with Kubernetes\n\nPotential Mentors\n-----------------\n\n1.  Jun Liu, Apache Dubbo PMC Chair, [junliu@apache.org![Image 2](https://issues.apache.org/jira/images/icons/mail_small.gif)](mailto:junliu@apache.org)\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Jun Liu_, mail: liujun (at) apache.org\n\n_Project Devs_, mail:\n\n### [GSoC 2025 - Dubbo Admin traffic management feature](https://issues.apache.org/jira/browse/DUBBO-140)\n\nBackground and Goal\n-------------------\n\nDubbo is an easy-to-use, high-performance microservice framework that provides both RPC and rich enterprise-level traffic management features.\n\nThe community has been working on the improvement of Dubbo's traffic management abilities, to make it support rich features like traffic spliting, canary release, a/b testing, circuit breaker, mocking, etc. The complete traffic management architecture in Dubbo consists of two major parts, Control Plane and Data Plane. In Dubbo, Control Plane refers to Dubbo Admin, with source code in apache/dubbo-kubernetes. Dubbo Data Plane is implemented by Dubbo sdk (Java, Go, etc)\n\nThe traffic management rules Dubbo ueses now is compatible with the rules in Istio. That means the rules generated by Dubbo Admin and sent to SDK is Istio compatible rules. In this project, we need developers to work mainly on Dubbo Admin to make sure it generates and sends those rules correctly.\n\nRelevant Skills\n---------------\n\n1.  Familiar with Golang\n2.  Familiar with Service Mesh, istio and Microservice architectures\n3.  Familiar with Kubernetes\n\nPotential Mentors\n-----------------\n\n1.  Jun Liu, Apache Dubbo PMC Chair, [junliu@apache.org![Image 3](https://issues.apache.org/jira/images/icons/mail_small.gif)](mailto:junliu@apache.org)\n2.  dev@dubbo.apache.org\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Jun Liu_, mail: liujun (at) apache.org\n\n_Project Devs_, mail:\n\n### [GSoC 2025 - Enhancing Dubbo Python Serialization](https://issues.apache.org/jira/browse/DUBBO-144)\n\nBackground and Goal\n-------------------\n\nCurrently, Dubbo Python exposes a serialization function interface that requires users to implement their own serialization methods. For commonly used serialization formats such as JSON and Protobuf, users must manually configure them each time. To streamline this process, we aim to build a built-in serialization layer that provides support for these common serialization formats by default.\n\nGoal\n----\n\nWe recommend using [Pydantic](https://docs.pydantic.dev/latest/) to achieve this. Therefore, we expect the implementation to:\n\n1\\. an internal serialization layer based on Pydantic, with support for at least JSON and Protobuf.\n\n2\\. Leverage Pydantic's additional features, including data validation and other useful functionalities.\n\nRelevant Skills\n---------------\n\n1\\. Familiar with Python\n\n2\\. Familiar with RPC\n\nPotential Mentors\n-----------------\n\n1.  Albumen Kevin, Apache Dubbo PMC, albumenj@apache.org\n2.  dev@dubbo.apache.org\n    \n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Albumen Kevin_, mail: albumenj (at) apache.org\n\n_Project Devs_, mail:\n\n### [GSoC 2025 - Dubbo triple protocol for go language implementation](https://issues.apache.org/jira/browse/DUBBO-142)\n\nBackground and Goal\n-------------------\n\nDubbo is an easy-to-use, high-performance microservice framework that provides both RPC and rich enterprise-level traffic management features.\n\n1.  keep-alive\n2.  connection management\n3.  programming api\n4.  error code\n\nRelevant Skills\n---------------\n\n1.  Familiar with Golang\n2.  Familiar with RPC\n3.  Familiar HTTP/1/2/3 protocol\n\nPotential Mentors\n-----------------\n\n1.  Jun Liu, Apache Dubbo PMC Chair, [junliu@apache.org![Image 4](https://issues.apache.org/jira/images/icons/mail_small.gif)](mailto:junliu@apache.org)\n2.  dev@dubbo.apache.org\n    \n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Jun Liu_, mail: liujun (at) apache.org\n\n_Project Devs_, mail:\n\n### [GSoC 2025 - Dubbo Gradle IDL Plugin](https://issues.apache.org/jira/browse/DUBBO-145)\n\nBackground and Goal\n-------------------\n\nIn the API-First design paradigm, IDL (Interface Definition Language) and its corresponding generation tools have become essential. IDL files are the specifications for defining service interfaces, and generation tools can convert IDL files into executable code, thereby simplifying the development process and improving efficiency.  \nCurrently, Apache Dubbo only provides a Maven IDL generation plugin, lacking a Gradle plugin. This brings inconvenience to developers using Gradle to build projects.\n\nNecessity\n---------\n\n1.  **Unify Build Tools**: Gradle is the preferred build tool for Android projects and many Java projects. Providing a Dubbo Gradle IDL plugin can maintain the consistency of build tools and reduce the cost for developers to switch between different build tools.\n2.  **Simplify Configuration**: Gradle plugins can simplify the configuration and generation process of IDL files. Developers only need to add plugin dependencies and simple configurations in the \\`build.gradle\\` file to complete the generation of IDL files without manually executing complex commands.\n3.  **Integrate Development Process**: Gradle plugins can be better integrated with IDEs (Integrated Development Environments). Developers can directly execute Gradle tasks in the IDE, thereby realizing the automatic generation of IDL files and improving development efficiency.\n\nImplementation Plan\n-------------------\n\n1.  **Plugin Development**: Develop a Gradle plugin that encapsulates the Dubbo IDL generation tool and provides a concise configuration interface.\n2.  **Configuration**: In the \\`build.gradle\\` file, developers can configure parameters such as the path of the IDL file and the directory of the generated code.\n3.  **Task**: The plugin provides a Gradle task for executing the generation of IDL files. Developers can execute the task through the command line or the IDE.\n4.  **Dependency Management**: The plugin can automatically manage the dependencies of the Dubbo IDL generation tool, ensuring that developers do not need to manually download and configure it.\n\nExpected Results\n----------------\n\n*   Developers can use Gradle to build Dubbo projects and easily generate the code corresponding to the IDL.\n*   Simplify the configuration and generation process of IDL files, and improve development efficiency.\n*   Better integration with IDEs to achieve automatic generation of IDL files.\n\nPotential Mentors\n-----------------\n\n1.  Albumen Kevin, Apache Dubbo PMC, albumenj@apache.org\n2.  dev@dubbo.apache.org\n    \n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Albumen Kevin_, mail: albumenj (at) apache.org\n\n_Project Devs_, mail:\n\nSeata\n-----\n\n### [GSoC 2025 - Apache Seata(Incubating) Extend multi-raft cluster mode](https://issues.apache.org/jira/browse/GSOC-274)\n\n#### Description\n\n**Synopsis**\n\nThe current Apache Seata Server supports the Raft cluster mode, but the performance and throughput of the cluster are significantly limited due to the single leader in a single Raft group. Therefore, the goal is to extend Seata Server to support multi-raft capability.\n\n**Benefits to Community**\n\nDue to the characteristics of Raft, requests are processed on the leader node and the results are submitted to the followers through the Raft consensus protocol. As a result, a significant amount of computational load is placed on the leader node, while followers only need to receive the final computed result. This causes the CPU, memory, and other metrics of the leader to be much higher than those of the followers. Additionally, the throughput of a single leader is limited by the machine configuration of the highest-spec node in the cluster, making it difficult to balance the traffic effectively. Therefore, supporting multi-raft would make the load distribution more balanced across all nodes in the cluster, improving throughput and performance, while also reducing the waste of machine resources.\n\n**Deliverables**\n\nThe expected delivery goal is to apply the multi-raft capability of the sofa-jraft component to Seata Server through detailed learning and practice\n\nThe step expected are the following:\n\n*   Learning and using the sofa-jraft component\n\n*   Understanding and practicing the transaction grouping capability in Seata\n\n*   Gaining a certain level of understanding of Seata's communication protocol\n\n*   Gaining a certain level of understanding of Seata's storage model, especially the Raft mode\n\n*   Ensuring compatibility between different versions\n\n**Useful links**\n\n*   [seata-raft-detailed-explanation](https://seata.apache.org/blog/seata-raft-detailed-explanation)\n\n*   [transaction-group](https://seata.apache.org/docs/next/user/txgroup/transaction-group)\n\n*   [sofa-jraft](https://www.sofastack.tech/en/projects/sofa-jraft/overview/)\n\n#### **Mentor**\n\n*   *   Mentor: Jianbin Chen, Apache Seata(Incubating) PPMC Member [jianbin@apache.org![Image 5](https://issues.apache.org/jira/images/icons/mail_small.gif)](mailto:jianbin@apache.org)\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Jianbin Chen_, mail: jianbin (at) apache.org\n\n_Project Devs_, mail: dev (at) seata.apache.org\n\n### [GSoC 2025 - Apache Seata(Incubating)Unlocking the Power of Metadata in Apache Seata From Load Balancing to Advanced Routing](https://issues.apache.org/jira/browse/GSOC-297)\n\n**Synopsis**\n\nCurrently, Apache Seata relies on a registry (e.g., Nacos, Zookeeper, Eureka, Etcd3, Consul, Seata Naming Server) for service discovery and load balancing. However, the existing registry mechanism lacks support for custom metadata, which limits the flexibility of client-side load balancing strategies. For example, clients cannot dynamically adjust traffic distribution based on server-side metadata such as weight, region, or version. This project aims to enhance the registry module in Apache Seata by adding metadata support and enabling clients to implement advanced load balancing strategies based on this metadata.\n\n**Improved Load Balancing Flexibility**: By allowing Seata Server instances to register custom metadata (e.g., weight, region, version), clients can implement more sophisticated load balancing strategies, such as weighted round-robin, zone-aware routing, or version-based routing. This ensures better resource utilization and improved system performance.\n\n**Enhanced Scalability**: With metadata-driven load balancing, Seata can better handle large-scale deployments by distributing traffic more intelligently across server instances. For example, high-traffic regions can be assigned more resources, while low-traffic regions can operate with minimal overhead.\n\n**Better Resource Utilization**: Metadata such as server weight or capacity can help clients avoid overloading specific instances, leading to more balanced resource usage across the cluster.\n\n**Extensibility**: The addition of metadata support opens the door for future enhancements, such as dynamic traffic shaping, A/B testing, or canary deployments.\n\n#### **Deliverables**\n\nThe expected deliverables for this project include:  \n**Registry Metadata Support**:\n\n*   Extend the registry module (e.g., Nacos, Zookeeper, Eureka, Etcd3, Consul, Seata Naming Server) to allow Seata Server instances to register custom metadata (e.g., weight, region, version).\n\n*   Ensure backward compatibility with existing registry implementations.\n\n**Client-Side Load Balancing Enhancements**:\n\n*   Implement a metadata-aware load balancing mechanism in the Seata client (TM/RM).\n\n*   Provide built-in load balancing strategies (e.g., weighted random, zone-aware) and allow users to plug in custom strategies via SPI.\n\n**Documentation and Testing**:\n\n*   Update the Seata documentation to explain how to configure and use metadata for load balancing.\n\n*   Write unit tests and integration tests to validate the new functionality\n\n#### **Steps Expected**\n\n**1.Understand Seata's Registry Mechanism**:\n\n*   Study how Seata integrates with various registries (e.g., Nacos, Zookeeper, Eureka, Etcd3, Consul, Seata Naming Server).\n\n*   Identify the current limitations in metadata support and load balancing.\n\n**2.Extend Registry Module**:\n\n*   Modify the registry module to allow Seata Server instances to register custom metadata.\n\n*   Ensure the metadata is propagated to clients during service discovery.\n\n**3.Implement Metadata-Aware Load Balancing**:\n\n*   Enhance the client-side load balancing logic to consider metadata (e.g., weight, region) when selecting a server instance.\n\n*   Provide built-in strategies (e.g., weighted random, zone-aware) and support custom strategies via SPI.\n\n**4.Ensure Compatibility and Performance**:\n\n*   Test the new functionality with different registry implementations (e.g., Nacos, Zookeeper, Eureka, Etcd3, Consul, Seata Naming Server).\n\n*   Optimize performance to minimize the overhead of metadata processing.\n\n**5.Documentation and Testing**:\n\n*   Write clear documentation on how to configure and use the new metadata and load balancing features.\n\n*   Develop comprehensive unit tests and integration tests.\n\n#### **Useful Links**\n\n*   [Apache Seata Documentation](https://seata.apache.org/)\n\n*   [Nacos](https://nacos.io/)\n\n*   [Zookeeper](https://zookeeper.apache.org/)\n\n*   [Eureka](https://github.com/Netflix/eureka)\n\n*   [Consul](https://www.consul.io/)\n\n*   [Etcd3](https://etcd.io/)\n\n*   [Seata Naming Server](https://seata.apache.org/docs/next/user/registry/namingserver)\n\n*   [Load Balancing Strategies](https://en.wikipedia.org/wiki/Load_balancing_(computing))\n\n#### **Mentor**\n\n**Mentor: Jiangke Wu, Apache Seata(Incubating) PPMC Member [xingfudeshi@apache.org![Image 6](https://issues.apache.org/jira/images/icons/mail_small.gif)](mailto:jianbin@apache.org)**\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Jiangke Wu_, mail: xingfudeshi (at) apache.org\n\n_Project Devs_, mail: dev (at) seata.apache.org\n\nKvrocks\n-------\n\n### [\\[GSOC\\]\\[Kvrocks\\] Improve the controller UI](https://issues.apache.org/jira/browse/GSOC-275)\n\n### Background\n\nApache Kvrocks is a distributed key-value NoSQL database that uses RocksDB as its storage engine and is compatible with Redis protocol.\n\nIn the past, basic Web UI capabilities have been provided for Apache Kvrocks Controller, including features such as cluster creation and migration. In the future, we aim to offer a better and more modern UI experience, also enhancing centralized visualization capabilities.\n\n### Objectives\n\nThe key objectives of the project include the following:\n\n*   Refactor the existing UI pages\n\n*   Enhance the visualization capabilities for cluster migration\n\n*   Provide a cluster Overview dashboard\n\n**Recommend Skills**\n\n1.  Familiar with next.js & tailwind\n2.  Have a basic understanding of RESTFul\n3.  Have an experience of Apache Kvrocks\n\nMentor: Hulk Lin, Apache Apache Kvrocks PMC,  hulk@apache.org  \nMailing List: dev@kvrocks.apache.org  \n_Please leave comments if you want to be a mentor_\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Hulk Lin_, mail: hulk (at) apache.org\n\n_Project Devs_, mail: dev (at) kvrocks.apache.org\n\n### [\\[GSOC\\]\\[Kvrocks\\] Support database backup to cloud storage](https://issues.apache.org/jira/browse/GSOC-276)\n\nBackgroud:\n----------\n\nKvrocks is a key-value database that provides a Redis-compatible API on top of RocksDB. Currently, Kvrocks lacks a built-in mechanism for database backup to cloud storage, which is crucial for data durability, disaster recovery, and scalability in cloud environments.\n\nThis project aims to implement a robust backup system that allows users to store Kvrocks backups directly in cloud storage services such as Amazon S3, Google Cloud Storage, and/or Azure Blob Storage. The solution will integrate with the existing Kvrocks backup and restore mechanisms while ensuring efficient and secure data transfer.\n\nDeliverables:\n-------------\n\n1.  **Cloud Storage Integration:** Implement backup storage support for Amazon S3, Google Cloud Storage, and Azure Blob Storage using SDKs, REST APIs or libraries (e.g. Apache OpenDAL).\n\n1.  **Backup & Restore Commands:** Extend Kvrocks’ backup functionality to allow exporting and importing database snapshots from cloud storage.\n\n1.  **Configuration & Authentication:** Provide user-configurable options to specify storage credentials and backup parameters.\n\n1.  **Incremental Backup Support (Stretch Goal):** Optimize storage usage by implementing differential or incremental backup capabilities.\n\n1.  **Documentation & Tests:** Comprehensive documentation and test coverage to ensure reliability and ease of use.\n\nRecommended Skills:\n-------------------\n\n1.  Good at coding in C++;\n2.  Knowledge about database internals and cloud storage;\n3.  Knowledge about Kvrocks or Redis.\n\nMentor: Mingyang Liu, Apache Kvrocks PMC member,  twice@apache.org  \nMailing List: dev@kvrocks.apache.org\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Mingyang Liu_, mail: twice (at) apache.org\n\n_Project Devs_, mail: dev (at) kvrocks.apache.org\n\nBeam\n----\n\n### [Enhancing Apache Beam JupyterLab Sidepanel for JupyterLab 4.x and Improved UI/UX](https://issues.apache.org/jira/browse/GSOC-277)\n\nThe Apache Beam JupyterLab Sidepanel provides a valuable tool for interactive development and visualization of Apache Beam pipelines within the JupyterLab environment. This project aims to significantly enhance the sidepanel by achieving full compatibility with the latest JupyterLab 4.x release and implementing substantial UI/UX improvements. This will ensure seamless integration with modern JupyterLab workflows and provide a more intuitive and user-friendly experience for Apache Beam developers.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_XQ Hu_, mail: xqhu (at) apache.org\n\n_Project Devs_, mail: dev (at) beam.apache.org\n\n### [Beam YAML ML, Iceberg, and Kafka User Accessibility](https://issues.apache.org/jira/browse/GSOC-278)\n\n[Apache Beam's YAML DSL](https://beam.apache.org/releases/yamldoc/current/) provides a powerful and declarative way to define data processing pipelines. However, its adoption for complex use cases like Machine Learning (ML) and Managed IO (specifically Apache Iceberg and Kafka) is hindered by a lack of comprehensive documentation and practical examples. This project aims to significantly improve the Beam YAML documentation and create illustrative examples focused on ML workflows and Iceberg/Kafka integration, making these advanced features more accessible to users.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_XQ Hu_, mail: xqhu (at) apache.org\n\n_Project Devs_, mail: dev (at) beam.apache.org\n\n### [Beam ML Vector DB/Feature Store integrations](https://issues.apache.org/jira/browse/GSOC-279)\n\nApache Beam's Python SDK provides a powerful way to define data processing pipelines. In particular, many users want to use Beam for machine learning use cases like feature generation, embedding generation, and retrieval augmented generation (RAG). Today, however, Beam integrates with a relatively limited set of feature stores and vector DBs for these use cases. This project aims to build out a rich ecosystem of connectors to systems like Pinecone and Tecton to enable these ML use cases.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Danny McCormick_, mail: damccorm (at) apache.org\n\n_Project Devs_, mail: dev (at) beam.apache.org\n\n### [Enhance lineage support in Beam](https://issues.apache.org/jira/browse/GSOC-294)\n\nApache Beam provides a powerful way to define data processing pipelines. However, it is increasingly important for users to be able to track how their data is moving through Beam so that they can make informed choices on how they manage their data at the source or sink of their pipeline. To solve for this, we have recently introduced data lineage in Beam - [https://en.wikipedia.org/wiki/Data\\_lineage](https://en.wikipedia.org/wiki/Data_lineage) - to this point support is still relatively limited though.\n\nFor this project, the focus would be on adding broader lineage support to Beam. This could include:\n\n*   adding column level lineage to more transforms\n*   adding direct runner support for lineage graphs ([https://github.com/apache/beam/issues/33980)](https://github.com/apache/beam/issues/33980))\n*   Integrating Beam with Open Lineage ([https://github.com/apache/beam/issues/33981](https://github.com/apache/beam/issues/33981))\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Danny McCormick_, mail: damccorm (at) apache.org\n\n_Project Devs_, mail: dev (at) beam.apache.org\n\nRocketMQ\n--------\n\n### [Optimizing Apache RocketMQ's POP Orderly Consumption Process](https://issues.apache.org/jira/browse/GSOC-280)\n\nBackground\n----------\n\nApache RocketMQ is a distributed messaging and streaming platform that supports various messaging protocols. One of the key features of RocketMQ is its orderly message consumption capability, which guarantees that messages are processed in the order they are sent. However, there are existing issues with the POP Orderly consumption process that need to be addressed to enhance its reliability and performance.\n\n Current Challenges\n-------------------\n\nCurrently, the POP Orderly feature faces several shortcomings, particularly in scenarios where network instability leads to the loss of the attemptId carried by the consumer from the previous round. This issue can result in message consumption getting stuck until the acknowledgment response (ack) for the previous message pull times out. Such situations hinder the efficient processing of messages and reduce the overall effectiveness of the messaging system.\n\n Objectives\n-----------\n\nThe primary objectives of this project are as follows:  \n●Refactor the POP Orderly Code: Analyze and redesign the existing codebase to improve its structure, maintainability, and performance.  \n●Optimize Performance: Implement performance enhancements that allow the POP Orderly feature to cope with network fluctuations and reduce the likelihood of consumption halting.  \n●Elegant Process Resolution: Develop a more graceful approach to handling the issue of consumption stalling, ensuring that the system can recover more smoothly from failures.\n\n Recommended Skills\n-------------------\n\n1\\. Proficiency in Java programming.  \n2\\. Strong understanding of concurrent programming.  \n3\\. Excellent logical thinking and problem-solving skills.  \n4\\. Familiarity with message queue systems, particularly Apache RocketMQ.\n\nMentor\n------\n\nRongtong Jin, Apache RocketMQ PMC, jinrongtong@apache.org\n\nPotential Mentor\n\nJuntao Ji, 3160102420@zju.edu.cn\n\nDifficulty: Major  \nProject Size: ~350 hours (large)\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Rongtong Jin_, mail: jinrongtong (at) apache.org\n\n_Project Devs_, mail: dev (at) rocketmq.apache.org\n\n### [Refactoring the RocketMQ Dashboard UI and Enhancing Usability](https://issues.apache.org/jira/browse/GSOC-282)\n\nBackground\n----------\n\nApache RocketMQ is renowned as a cloud-native messaging and streaming platform, enabling the creation of event-driven applications with simplicity and flexibility. The RocketMQ Dashboard is a crucial component that provides users with insight into system performance and client interactions through intuitive graphs and statistical data. Despite its fundamental role, the current user interface (UI) of the RocketMQ Dashboard is outdated, affecting user experience and interaction efficiency. Additionally, while the Dashboard offers valuable functionalities, there is a pressing need to enhance its usability and ensure robust security. This project aims to refactor the RocketMQ Dashboard by redesigning its UI with a more contemporary and user-friendly approach, improving overall usability, and introducing effective security measures to safeguard data and user interactions.\n\nRelevant Skills\n---------------\n\n*   Strong Java development skills.\n*   Experience with modern front-end technologies and frameworks\n*   Proficiency in Spring Boot development.\n*   Understanding of UX/UI design principles. - Knowledge of security best practices in web applications.\n*   A keen interest in open-source projects and a willingness to learn and adapt.\n\nTasks\n-----\n\n*   Launch and experiment with the RocketMQ Dashboard to understand current functionalities.\n*   Refactor the UI of the RocketMQ Dashboard to align with modern user interface standards, ensuring it is intuitive and visually appealing.  \n*   Improve usability by streamlining workflows, enhancing navigation, and incorporating responsive design. \n*   Integrate security features to protect user data, prevent unauthorized access, and mitigate potential vulnerabilities.\n*   Maintain compatibility with existing RocketMQ functionalities while focusing on enhancements. \n\nLearning Material\n-----------------\n\n*   RocketMQ HomePage: [https://rocketmq.apache.org](https://rocketmq.apache.org/)(\\[https://rocketmq.apache.org|https://rocketmq.apache.org/\\])\n*   RocketMQ GitHub Repository: [https://github.com/apache/rocketmq](https://github.com/apache/rocketmq)(\\[https://github.com/apache/rocketmq\\]) \n*   RocketMQ Dashboard GitHub Repository: [https://github.com/apache/rocketmq-dashboard](https://github.com/apache/rocketmq-dashboard)(\\[https://github.com/apache/rocketmq-dashboard\\]) \n\nMentor\n------\n\nRongtong Jin, Apache RocketMQ PMC, jinrongtong@apache.org\n\nPotential Mentor\n\nJuntao Ji, 3160102420@zju.edu.cn\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Rongtong Jin_, mail: jinrongtong (at) apache.org\n\n_Project Devs_, mail: dev (at) rocketmq.apache.org\n\nSkyWalking\n----------\n\n### [SkyWalking BanyanDB Extend remote.FS with Object Storage Support for AWS, Google Cloud, and Azure](https://issues.apache.org/jira/browse/GSOC-281)\n\n**Overview:**  \nThe current implementation of the remote.FS interface only supports a local file system (via the implementation in local.go). This GSOC2025 project proposes to extend remote.FS with popular object storage services—namely AWS S3, Google Cloud Storage, and Azure Blob Storage. This enhancement will allow the project to support robust cloud-based backup and restore operations in addition to local storage.\n\n**Proposed Features:**\n\n1.  **AWS S3 Implementation:**\n    *   Implement methods for Upload, Download, List, and Delete operations using the AWS S3 API.\n2.  **Google Cloud Storage Implementation:**\n    *   Provide a module that integrates with Google Cloud Storage to perform similar operations.\n3.  **Azure Blob Storage Implementation:**\n    *   Develop functionality to access and manage Azure Blob Storage via the remote.FS interface.\n\n**Implementation Details:**\n\n*   **Interface Compliance:**  \n    Each object storage implementation must adhere to the remote.FS interface defined in remote.go.\n\n*   **Error Handling & Resilience:**  \n    Implement robust error handling, logging, and retry mechanisms to ensure reliable operations across different cloud services.\n\n*   **Testing:**  \n    Develop comprehensive unit and integration tests to cover edge cases and guarantee compatibility and stability.\n\n*   **Documentation:**  \n    Update the project documentation to detail configuration, deployment, and usage of each cloud storage option.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Hongtao Gao_, mail: hanahmily (at) apache.org\n\n_Project Devs_, mail: dev (at) skywalking.apache.org\n\nHugeGraph\n---------\n\n### [\\[GSoC\\]\\[HugeGraph\\] Implement Agentic GraphRAG Architecture](https://issues.apache.org/jira/browse/GSOC-283)\n\nApache [HugeGraph](https://hugegraph.apache.org/)(incubating) is a fast-speed and highly-scalable [graph database](https://en.wikipedia.org/wiki/Graph_database)/computing/AI ecosystem. Billions of vertices and edges can be easily stored into and queried from HugeGraph due to its excellent OLTP/OLAP ability.\n\nWebsite: [https://hugegraph.apache.org/](https://hugegraph.apache.org/)  \nGitHub:\n\n*   [https://github.com/apache/incubator-hugegraph/](https://github.com/apache/incubator-hugegraph/)\n*   [https://github.com/apache/incubator-hugegraph-ai/](https://github.com/apache/incubator-hugegraph/)  \n     \n    \n    Description\n    -----------\n    \n\nCurrently, we have implemented a basic GraphRAG that relies on fixed processing workflows (e.g., knowledge retrieval & graph structure updates using the same execution pipeline), leading to insufficient flexibility and high overhead in complex scenarios. The proposed task introduces an Agentic architecture based on the principles of \"dynamic awareness, lightweight scheduling, concurrent execution,\" focusing on solving the following issues:\n\n1.  **Rigid Intent Recognition**: Existing systems cannot effectively distinguish between simple retrievals (e.g., entity queries) and complex operations (e.g., multi-hop reasoning), often defaulting to BFS-based template subgraph searches.\n2.  **Coupled Execution Resources**: Memory/computational resources are not isolated based on task characteristics, causing long-tail tasks to block high-priority requests.\n3.  **Lack of Feedback Mechanisms**: Absence of self-correction capabilities for erroneous operations (e.g., automatically switching to similar vertices/entities after path retrieval failures).\n\nThe task will include three core parts:\n\n**1\\. Dynamic Awareness Layer**\n\n*   Implement an LLM-based real-time (as of February 14, 2025) intent classifier that categorizes tasks (L1 simple retrieval/L2 path reasoning/L3 graph computation/L4+ etc.) based on semantic features (verb types/entity complexity/temporal modifiers).\n*   Build a lightweight operation cache to generate feature hashes for high-frequency requests, enabling millisecond-level intent matching.\n\n**2\\. Task Orchestration Layer**\n\n*   Introduce a suitable workflow/taskflow framework emphasizing low coupling, high performance, and flexibility.\n*   Adopt a preemptive scheduling mechanism allowing high-priority tasks to pause non-critical phases of low-priority tasks (e.g., suspending subgraph preloading without interrupting core computations).\n\n**3\\. Concurrent Execution**\n\n*   Decouple traditional RAG pipelines into composable operations (entity recall → path validation → context enhancement → result refinement), with dynamic enable/disable support for each component.\n*   Implement automatic execution engine degradation, triggering fallback strategies upon sub-operation failures (e.g., switching to alternative methods if Gremlin queries timeout).\n\n**Recommended Skills**\n----------------------\n\n1.  Proficiency in Python and familiarity with at least one open/closed-source LLM.\n2.  Experience with one LLM RAG/Agent framework like LangGraph/RAGflow/LLamaindex/Dify.\n3.  Knowledge of LLM optimization techniques and RAG construction (KG extraction/construction experience is a plus).\n4.  Strong algorithmic engineering skills (problem abstraction, algorithm research, big data processing, model tuning).\n5.  Familiarity with VectorDB/Graph/KG/HugeGraph read-write workflows and principles.\n6.  Understanding of graph algorithms (e.g., community detection, centrality, PageRank) and open-source community experience preferred.\n\n**Task List**\n\n*   Develop a hierarchical triggering mechanism for the intent classifier to categorize L1~LN tasks within milliseconds (accuracy \\>90%).\n\n*   Semi-automatically generate Graph Schema/extraction prompts.\n\n*   Support dynamic routing and query decomposition.\n\n*   Design an execution trace tracker to log micro-operation resource consumption and generate optimization reports.\n\n*   Enhance retrieval with graph algorithms: Apply node importance evaluation, path search, etc., to optimize knowledge recall.\n\n*   Implement a dialogue memory management module for context-aware state tracking and information reuse.\n\n### Size\n\n*   Difficulty: Hard\n\n*   Project size: ~350 hours (full-time/large)\n\nPotential Mentors\n-----------------\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Imba Jin_, mail: jin (at) apache.org\n\n_Project Devs_, mail:\n\nDolphinScheduler\n----------------\n\n### [Enhancing Apache DolphinScheduler with Generalized OIDC Authentication](https://issues.apache.org/jira/browse/GSOC-284)\n\nBackground\n----------\n\nApache DolphinScheduler is a distributed and extensible workflow scheduler platform designed to orchestrate complex data processing tasks. It provides a user-friendly interface for defining, scheduling, and monitoring workflows, making it easier to manage and automate data pipelines. DolphinScheduler supports various types of tasks, including shell scripts, SQL queries, and custom scripts, and integrates seamlessly with popular big data ecosystems.\n\nCurrently, the Apache DolphinScheduler system supports user login via Password, LDAP, Casdoor SSO, and OAuth. However, as a data platform, it frequently needs to integrate with enterprise - internal user accounts to achieve unified identity authentication, which is crucial for ensuring system security and unified user account management. The existing implementation of Casdoor has a high degree of dependence on the Casdoor project, and the OAuth implementation lacks universality and flexibility.\n\nOur objective is to implement a more generalized OIDC (OpenID Connect) login authentication mechanism. This will enable users to make better use of unified login authentication. Moreover, popular open source login authentication projects like Dexidp, Keycloak, and OAuthProxy all support OIDC. By supporting OIDC, users can integrate with both internal and third-party login authentication methods, such as Feishu Login and WeChat Work Login.\n\nRelevant Skills\n---------------\n\nStrong proficiency in Java development.  \nExperience in modern frontend technologies and frameworks.  \nHighlevel expertise in Spring Boot development.  \nThorough familiarity with OIDC and OAuth2 protocols.  \nKeen interest in opensource projects and eagerness to learn and adapt.\n\nTasks\n-----\n\nInitiate and conduct experiments with Apache DolphinScheduler to comprehensively understand its current functionalities.  \nImplement and support a more generalized OIDC (OpenID Connect) login authentication mechanism.  \nCompose corresponding E2E test cases.  \nCreate corresponding documentation for third-party login integrations, covering Keycloak, Dexidp, OAuthProxy, as well as Feishu Login and WeChat Work Login.  \nOptimize the UI of the Apache DolphinScheduler login page.  \nEnsure compatibility with the existing functionalities of Apache DolphinScheduler during the process of focusing on enhancements.\n\nLearning Material\n-----------------\n\nApache DolphinScheduler HomePage: [https://dolphinscheduler.apache.org](https://dolphinscheduler.apache.org/)  \nApache DolphinScheduler GitHub Repository: [https://github.com/apache/dolphinscheduler](https://github.com/apache/dolphinscheduler)  \nSprint OAuth 2.0 Client: [https://docs.spring.io/spring-security/reference/reactive/oauth2/client/index.html](https://docs.spring.io/spring-security/reference/reactive/oauth2/client/index.html)\n\npac4j OIDC: [https://www.pac4j.org/docs/clients/openid-connect.html](https://www.pac4j.org/docs/clients/openid-connect.html)  \nOIDC (OpenID Connect): [https://openid.net/developers/how-connect-works/](https://openid.net/developers/how-connect-works/)\n\nMentor\n------\n\nGallardot, Apache DolphinScheduler committer, [gallardot@apache.org![Image 7](https://issues.apache.org/jira/images/icons/mail_small.gif)](mailto:gallardot@apache.org)\n\nSbloodyS, Apache DolphinScheduler PMC, [zihaoxiang@apache.org![Image 8](https://issues.apache.org/jira/images/icons/mail_small.gif)](mailto:zihaoxiang@apache.org)\n\nDifficulty: Medium  \nProject Size: ~150 hours (medium)\n\n**Difficulty:** Major\n\n**Project size:** ~175 hour (medium)\n\n**Potential mentors:**\n\n_Hengliang Tan_, mail: gallardot (at) apache.org\n\n_Project Devs_, mail: dev (at) dolphinscheduler.apache.org\n\nLucene.NET\n----------\n\n### [Apache Lucene.NET Replicator and Dependency Injection Enhancements](https://issues.apache.org/jira/browse/GSOC-286)\n\n**Background and Goal**\n\nApache Lucene.NET is a .NET port of the Apache Lucene search engine (originally written in Java). This powerful library enables indexing and searching of documents with custom queries, making it a core component in many production environments. With over 100 million NuGet downloads, Lucene.NET is utilized in diverse scenarios, from local search functionality in mobile apps to supporting large-scale cloud infrastructures.\n\nLucene.NET already provides a foundation for replicating a search index from a primary node to one or more replica nodes, enabling High Availability (HA) and scalability through load balancing. Currently, our Lucene.Net.Replicator.AspNetCore project offers minimal replication support for ASP.NET Core servers, but it remains unpublished on NuGet and lacks the robustness required for most use cases. Your focus for this project will be to enhance and finalize the ASP.NET Core library, ensuring a seamless user experience by adhering to best practices and making replication setup as straightforward as possible – ideally requiring just one line of code.\n\nAdditionally, users may need replication support for applications outside ASP.NET Core, such as cloud-based distributed architectures, Windows services, or command-line tools running on Linux. To address this, we propose creating modular intermediate libraries using Microsoft.Extensions.DependencyInjection.Abstractions, enabling flexible and reusable replication configurations. This approach should also ensure that essential components like IndexWriter and IndexReader are configured in a straightforward and user-friendly manner.\n\nYour task will also include creating one or more sample projects that demonstrate how to effectively use the enhanced replication functionality. These projects should serve as practical, real-world examples for the community, showcasing best practices and ease of use. Additionally, you will be responsible for thoroughly testing the code changes to ensure they work as intended in real-world scenarios. This includes writing comprehensive unit tests to guarantee the reliability and quality of the solution.\n\nWe plan for this to be a hands-on mentorship, and we will set up any infrastructure for you. As a contributor, your responsibilities will include analyzing the problem, developing a detailed plan, refining it with input from the project team, and collaborating regularly to implement the solution through pull requests and code reviews.\n\n**Relevant Skills**\n\n*   Familiarity with C# and unit testing\n*   Strong grasp of design patterns and practices, such as dependency injection and i.e. the fluent builder and abstract factory patterns\n*   Basic understanding of HTTP(S) and networking\n*   Not required, but good to have:\n    *   Familiarity with ASP.NET Core 5 or later\n    *   Understanding of distributed architectures\n    *   Familiarity with Lucene(.NET) search indexes\n\n**Difficulty:** Normal\n\n**Project size:** ~175 hour (medium)\n\n**Potential mentors:**\n\n_Paul Irwin_, mail: paulirwin (at) apache.org\n\n_Project Devs_, mail: dev (at) lucenenet.apache.org\n\nCloudStack\n----------\n\n### [Apache CloudStack DRS improvements](https://issues.apache.org/jira/browse/GSOC-285)\n\nAs a Operator I would like to have the loads on my systems more evenly/centrally distributed. At the moment there is a simple DRS for clusterwide distribution of loads, this is however not applying zone wide distribution or based on automated queries/improvements.\n\nIn addition we should add historic data for the VM in planning possible migrations.  \nAt the moment allocated metrics are used. An first improvement would be to use actual metrics.\n\nref: cloudstack issue: [https://github.com/apache/cloudstack/issues/10397](https://github.com/apache/cloudstack/issues/10397)\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Daan_, mail: dahn (at) apache.org\n\n_Project Devs_, mail: dev (at) cloudstack.apache.org\n\n### [verification of LDAP connection](https://issues.apache.org/jira/browse/GSOC-287)\n\nWhen a new ldap connection is added there is no diagnostics to verify the validity/usability of the connection, making trouble shooting troublesome. This issue aims to facilitate ldap configuration.\n\nref. cloudstack issue: [https://github.com/apache/cloudstack/issues/6934](https://github.com/apache/cloudstack/issues/6934)\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Daan_, mail: dahn (at) apache.org\n\n_Project Devs_, mail: dev (at) cloudstack.apache.org\n\n### [SSL - LetsEncrypt the Console Proxy](https://issues.apache.org/jira/browse/GSOC-288)\n\n**Difficulty:** Major\n\n**Project size:** ~175 hour (medium)\n\n**Potential mentors:**\n\n_Daan_, mail: dahn (at) apache.org\n\n_Project Devs_, mail: dev (at) cloudstack.apache.org\n\n### [Autodetect IPs used inside the VM on L2 networks](https://issues.apache.org/jira/browse/GSOC-289)\n\nWith regards to IP info reporting, Cloudstack relies entirely on it's DHCP data bases and so on. When this is not available (L2 networks etc) no IP information is shown for a given VM.\n\nI propose we introduce a mechanism for \"IP autodetection\" and try to discover the IPs used inside the machines by means of querying the hypervisors. For example with KVM/libvirt we can simply do something like this:  \n {{root@fedora35 ~\\]# virsh domifaddr win2k22 --source agent  \nName MAC address Protocol Address  \n\\-------------------------------------------------------------------------------  \nEthernet 52:54:00:7b:23:6a ipv4 192.168.0.68/24  \nLoopback Pseudo-Interface 1 ipv6 ::1/128\n\n*   \\- ipv4 127.0.0.1/8}}The above command queries the qemu-guest-agent inside the Windows VM. The VM needs to have the qemu-guest-agent installed and running as well as the virtio serial drivers (easily done in this case with [virtio-win-guest-tools.exe](https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/virtio-win-0.1.215-2/virtio-win-guest-tools.exe) ) as well as a guest-agent socket channel defined in libvirt.\n    \n\nref. cloudstack issue: [https://github.com/apache/cloudstack/issues/7142](https://github.com/apache/cloudstack/issues/7142)\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Daan_, mail: dahn (at) apache.org\n\n_Project Devs_, mail: dev (at) cloudstack.apache.org\n\n### [eBPF-based Network Observability for CloudStack](https://issues.apache.org/jira/browse/GSOC-292)\n\nCloudStack’s network monitoring is mostly based on logs and external agents, making real-time traffic analysis difficult. This project will integrate **eBPF-based network observability** to capture per-VM traffic metrics, detect anomalies, and improve tenant isolation.\n\nBenefits to CloudStack\n----------------------\n\n*   **Enhanced security**: Detect suspicious activity at the kernel level.\n*   **Real-time traffic monitoring**: Gain deep insights into VM networking.\n*   **Better tenant isolation**: Identify cross-tenant traffic issues.\n\nDeliverables\n------------\n\n*   Develop eBPF probes to capture:\n    *   Per-VM network traffic metrics (packets, bytes, latency)\n    *   Connection tracking for detecting unauthorized access patterns\n    *   Packet drops and retransmission rates\n*   Expose network metrics via CloudStack’s API.\n*   Provide visualization through Prometheus/Grafana.\n*   Document setup, usage, and performance benchmarks.\n\nExpected Outcome\n----------------\n\nAn eBPF-based solution that improves network observability in CloudStack, providing security and performance insights with minimal resource usage.\n\n**Difficulty:** Major\n\n**Project size:** ~175 hour (medium)\n\n**Potential mentors:**\n\n_Daan_, mail: dahn (at) apache.org\n\n_Project Devs_, mail: dev (at) cloudstack.apache.org\n\n### [Enhancing CloudStack Monitoring with eBPF](https://issues.apache.org/jira/browse/GSOC-291)\n\nApache CloudStack currently relies on traditional monitoring tools, which may lack deep visibility into kernel-level events and networking performance. This project aims to integrate **eBPF-based monitoring** into CloudStack to provide lightweight, real-time performance analysis and security auditing.\n\nBenefits to CloudStack\n----------------------\n\n*   **Improved observability**: Gain fine-grained insights into VM performance metrics.\n*   **Lower overhead**: eBPF runs in the kernel and avoids the performance penalties of user-space monitoring tools.\n*   **Enhanced security auditing**: Detect and log anomalies in system behavior.\n\nDeliverables\n------------\n\n*   Implement eBPF programs to track:\n    *   VM CPU usage\n    *   Memory consumption\n    *   Disk I/O metrics\n    *   Network traffic analysis\n*   Develop a CloudStack-compatible API or CLI for retrieving eBPF-generated insights.\n*   Provide visualization support using Prometheus/Grafana.\n*   Write documentation for setup and usage.\n\nExpected Outcome\n----------------\n\nA robust eBPF-based monitoring solution integrated into CloudStack, offering real-time performance insights with minimal overhead.\n\nref. cloudstack issue: [https://github.com/apache/cloudstack/issues/10415](https://github.com/apache/cloudstack/issues/10415)\n\nThis project is marked as part-time, but the scope can be extended to full-time. This depends largely on whether the full amount of metrics to track is implemented or only one, as a proof of concept.\n\n**Difficulty:** Major\n\n**Project size:** ~175 hour (medium)\n\n**Potential mentors:**\n\n_Daan_, mail: dahn (at) apache.org\n\n_Project Devs_, mail: dev (at) cloudstack.apache.org\n\n### [\\[GSoC\\] \\[CloudStack\\] Improve CloudMonkey user experience by enhancing autocompletion](https://issues.apache.org/jira/browse/GSOC-295)\n\nSummary\n-------\n\nCurrently a lot of API parameters do not get auto-completed as cloudmonkey isn't able to deduce the probable values for those parameters based on the list APIs heuristics. A lot of these parameters are enums on CloudStack end and by finding a way to expose these and consume them on cloudmonkey side, we could improve the usability of the CLI greatly.\n\nBenefits to CloudStack\n----------------------\n\n*   Improved end user experience when using CLI\n*   Reduce incorrect inputs\n\nDeliverables\n------------\n\n*   Expose enums and all other relevant information that can be used to enhance auto-completion of parameters on CloudStack end -\n    *   May require framework level changes and changes to APIs\n*   Consume these exposed details on Cloudmonkey end\n\nDependent projects\n------------------\n\n[https://github.com/apache/cloudstack-cloudmonkey/](https://github.com/apache/cloudstack-cloudmonkey/)\n\nRef CloudStack Issue: [https://github.com/apache/cloudstack/issues/10442](https://github.com/apache/cloudstack/issues/10442)\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Pearl Dsilva_, mail: pearl11594 (at) apache.org\n\n_Project Devs_, mail: dev (at) cloudstack.apache.org\n\n### [add securitata integration to cloudstack](https://issues.apache.org/jira/browse/GSOC-296)\n\nCurrently, Cloudstack only has ACLs (in Advanced Networks) that as a layer of securing access to the networks (VPCs). However, these only operate in the Layer 3 and 4 of OSI Layer.\n\nIn todays day and age, where Cybersecurity threats become more advanced, complex and operate in Layer 7 OSI layer, there needs to be a way for Cloudstack to allow its own tenants to implement its own form of mature cybersecurity solution.\n\nThe problem all this while is that if a user is using a VPC or L2 Networks, 3rd party firewalls such as PFsense, FortinetVM Firwall etc cant be implemented effectively due to a lack of being able to set static routes that stays with the VR after it is recreated.\n\nThere needs to be a better option for users of cloudstack to implement a deeper form of cybersecurity to protect their workloads.\n\nref. github issue:  [https://github.com/apache/cloudstack/issues/10445](https://github.com/apache/cloudstack/issues/10445)\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Daan_, mail: dahn (at) apache.org\n\n_Project Devs_, mail: dev (at) cloudstack.apache.org\n\nStreamPipes\n-----------\n\n### [Extend visualization capabilities of Apache StreamPipes](https://issues.apache.org/jira/browse/GSOC-290)\n\nBackground\n----------\n\nApache StreamPipes is a self-service Industrial IoT toolbox which helps users to connect, analyze and exploit industrial data streams. StreamPipes offers a variety of tools which help users to interact with data from industrial sources such as PLCs. An adapter library allows to get real-time data from industrial controllers or other systems, a pipeline editor allows to build stream processing pipelines using either graphical or code-based flow modeling, and a data explorer allows to quickly create visualizations based on connected adapters.\n\n Current Challenges\n-------------------\n\nThe StreamPipes data explorer consists of a chart view, where users can create charts based on live data, and a dashboard view, where users can create live dashboards based on charts.\n\nThe data explorer provides a set of charts, which are mainly based on Apache ECharts. The currently available chart library includes time-series line/bar charts, heatmaps, scatter plots, density charts and others. To improve the user experience and add additional capabilities, we plan to extend this chart library with additional charts that are useful for industrial data analytics.\n\n Objectives\n-----------\n\nThe primary objectives of this project are as follows:\n\n*   Explore the Apache ECharts library and identify useful additional charts for industrial data analytics\n*   Improve the StreamPipes data explorer by adding new chart types using Apache ECharts\n*   Add a more advanced table visualization\n*   Extend existing charts with additional configurations (e.g., axis configurations, labels, data transformations)\n*   Add a data preview for all charts, which is shown below the actual chart in the chart view\n*   Design and implement end-to-end-tests using Cypress\n\n Recommended Skills\n-------------------\n\n1.  Proficiency in TypeScript programming + testing \n2.  Proficiency in Angular\n3.  Excellent logical thinking and problem-solving skills.\n4.  Good sense for beautifully looking user interfaces \n\nMentor\n------\n\nDominik Riemer, Apache StreamPipes PMC, riemer@apache.org\n\nDifficulty: Major  \nProject Size: ~350 hours (large)\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Dominik Riemer_, mail: riemer (at) apache.org\n\n_Project Devs_, mail: dev (at) streampipes.apache.org\n\nHertzBeat\n---------\n\n### [\\[GSOC\\]\\[HertzBeat\\] AI Agent Based on the MCP Protocol for Monitoring Info Interaction](https://issues.apache.org/jira/browse/GSOC-293)\n\nWebsite: [https://hertzbeat.apache.org/](https://hertzbeat.apache.org/)\n\nGithub: [http://github.com/apache/hertzbeat/](http://github.com/apache/hertzbeat/)\n\n\\***Background**\\*\n\nApache HertzBeat is an open-source real-time monitoring tool that supports a wide range of monitoring targets, including web services, databases, middleware, and more. It features high performance, scalability, and security.\n\nWith the advancement of artificial intelligence (AI) technologies, integrating AI with monitoring systems can significantly enhance their usability and interactivity. By developing an AI Agent based on the Model Context Protocol (MCP), we aim to enable conversational interaction for querying monitoring information, adding new monitoring tasks, and retrieving monitoring metrics. This will provide a more user-friendly and intelligent monitoring management experience.\n\n\\***Objectives**\\*\n\n1\\. Research and Implementation: Develop an AI Agent based on Apache HertzBeat and the MCP protocol to enable conversational interaction with users.\n\n2\\. Functional Implementation:\n\n*   Query Monitoring And Alarm Information: Allow users to query the status of monitoring targets (e.g., normal, abnormal) and retrieve metrics data (e.g., CPU usage, memory usage, response time), alarm data through conversational commands.\n*   Add New Monitoring Tasks: Enable users to add new monitoring targets (e.g., web services, databases, middleware) and configure alert thresholds via conversational commands.\n*   Retrieve Monitoring Metrics Data: Allow users to obtain metrics data for specific monitoring targets and support data visualization via conversational commands.\n\n\\***Requirements Analysis**\\*\n\n*   Apache HertzBeat: As the core backend for the monitoring system, it provides functions for data collection, storage, and management.\n\n*   MCP Protocol: An open protocol that enables seamless integration between LLM applications and external data sources and tools.\n\n*   Front-end Interaction: Develop a user-friendly interface that supports voice or text input and displays monitoring information and interaction results.\n\n\\***Recommended Skills**\\*\n\n*   Java + TypeScript: Apache HertzBeat is developed based on this technology stack. Therefore, mastering these technologies is crucial for integrating with HertzBeat.\n*   SpringAi: It is recommended to use SpringAi to build the AI agent.\n*   LLM + MCP: You need to have an understanding of LLM (Large Language Models) and the MCP protocol. SpringAi seem supports the MCP protocol or consider use the mcp-sdk directly.\n\n\\***Size**\\*\n\n*   Difficulty: Hard\n*   Project size: ~350 hours\n\n\\***Potential Mentors**\\*\n\n*   Chao Gong: gongchao@apache.org   \n     \n*   Shenghang Zhang: shenghang@apache.org\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Chao Gong_, mail: gongchao (at) apache.org\n\n_Project Devs_, mail: dev (at) hertzbeat.apache.org\n\nComdev GSOC\n-----------\n\n### [Apache Mahout Refactoring the Website](https://issues.apache.org/jira/browse/GSOC-298)\n\n**Synopsis**\n------------\n\nApache Mahout has been evolving, with a recent shift in focus toward **Quantum Computing (Qumat)**. However, the official website does not currently reflect this transition, making it difficult for developers and contributors to engage with Mahout’s new direction. Additionally, legacy components like **MapReduce and Samsara** are no longer actively developed but still occupy prominent space on the website.\n\nThis project aims to **refactor the Apache Mahout website** to:\n\n1.  **Bring Quantum Computing (Qumat) front and center** as the new core focus of the project.\n2.  **Deprecate outdated technologies (MapReduce and Samsara)** while keeping the documentation intact with clear deprecation warnings.\n3.  **Improve website structure, navigation, and content organization** to enhance accessibility and usability.\n\nBy executing these changes, this project will ensure that new and existing users can quickly access relevant information while keeping historical documentation available in a structured manner.\n\nA well-organized and up-to-date website is essential for any open-source project. This proposal offers multiple benefits to the Apache Mahout community:\n\n### **1\\. Highlighting Quantum Computing (Qumat)**\n\n*   Restructure the website so that **Qumat-related content is the primary focus**.\n*   Ensure that all documentation, blogs, and tutorials related to Qumat are **easily discoverable** from the homepage.\n\n### **2\\. Deprecating MapReduce and Samsara**\n\n*   Add **clear deprecation warnings** to pages related to MapReduce and Samsara.\n*   Ensure these technologies remain accessible for historical reference but **indicate that they are no longer actively maintained**.\n\n### **3\\. Improved Navigation and Accessibility**\n\n*   Design a **more intuitive navigation system** for easy exploration of different sections.\n*   Ensure smooth access to **documentation, blogs, and learning resources**.\n\n### **4\\. Updating Outdated Content**\n\n*   Perform a **full website audit** to identify obsolete articles, guides, and references.\n*   Refresh and rewrite content where necessary, focusing on Mahout’s latest advancements.\n\n### **5\\. Engaging New Contributors**\n\n*   A modern, user-friendly website will attract **more developers, researchers, and open-source contributors** to the project.\n\n**Deliverables**\n----------------\n\n### **1\\. Website Restructuring**\n\n*   Modify the **homepage and navigation bar** to prominently feature **Quantum Computing (Qumat)** as the main focus.\n*   Ensure Qumat-related documentation and blog posts are front and center.\n\n### **2\\. Deprecation of MapReduce and Samsara**\n\n*   Add **banner notifications** on all MapReduce and Samsara pages marking them as deprecated.\n*   Ensure clear explanations so users understand these technologies are no longer in active development.\n\n### **3\\. Content Review & Updates**\n\n*   Perform a **recursive LS audit** to identify outdated and redundant content.\n*   Update old blogs and articles to align with Mahout’s latest developments.\n\n### **4\\. Improved Website Navigation**\n\n*   Implement a **modern, responsive, and mobile-friendly** navigation system.\n*   Optimize loading speed and ensure smooth user experience.\n\n### **5\\. Documentation Enhancement**\n\n*   Ensure all **essential documentation is accessible from the homepage**.\n*   Improve the **readability and structure** of the docs.\n\n**Technical Details**\n---------------------\n\nThe project will utilize:\n\n*   **HTML, CSS, JavaScript** for website front-end improvements.\n*   **Modern front-end frameworks** (if required) to enhance UX/UI.\n*   **Shell scripting or Python** to perform a **recursive LS** audit of the website structure.\n*   **Version control via GitHub** for tracking changes and ensuring collaboration.\n\n**Expected Outcomes**\n---------------------\n\n✅ A **refactored website** that clearly emphasizes **Quantum Computing (Qumat)**.  \n✅ A **deprecated but accessible** archive for MapReduce and Samsara.  \n✅ An **updated and well-structured content repository** for Mahout users and contributors.  \n✅ An **intuitive, user-friendly website** that engages both new and existing users.\n\n**Timeline (12+ Weeks, Full-Time Commitment - 30 hrs/week)**\n------------------------------------------------------------\n\n*   Engage with mentors and the Mahout community.\n*   Gather feedback on website restructuring priorities.\n*   Set up the development environment and review existing website architecture.\n\n### **Phase 1: Planning & Initial Development (Weeks 3-6)**\n\n*   Redesign homepage and navigation bar to prioritize **Qumat**.\n*   Identify and start modifying MapReduce and Samsara pages with **deprecation warnings**.\n*   Conduct a **recursive LS audit** to locate outdated files and redundant content.\n\n### **Phase 2: Implementation & Testing (Weeks 7-10)**\n\n*   Implement the **new website navigation** and homepage.\n*   Update and restructure documentation and blog content.\n*   Optimize the website’s **file structure** based on LS audit findings.\n*   Conduct extensive testing for **responsiveness, accessibility, and performance**.\n\n### **Phase 3: Content Finalization & Refinement (Weeks 11-12+)**\n\n*   Finalize deprecation notices for MapReduce and Samsara.\n*   Ensure **all Qumat-related content is easily accessible**.\n*   Perform last-minute optimizations and bug fixes.\n*   Gather final feedback from the community and document all changes.\n\n🔹 **Total Timeline: 350+ hrs**\n\n**Why This Should Be a GSoC Project**\n-------------------------------------\n\nThis project directly aligns with **Google Summer of Code’s mission** to enhance open-source software. By modernizing the Apache Mahout website, we ensure that its **new focus on Quantum Computing (Qumat) is clearly reflected**, making it easier for developers and researchers to engage with Mahout’s latest advancements.\n\nAdditionally, this project is well-scoped for GSoC, combining **front-end development, content management, and structured auditing**—all crucial aspects for a website overhaul.\n\n**Mentorship & Feasibility**\n----------------------------\n\n*   The project has **clear, well-defined goals** and structured milestones.\n*   It will be mentored by an **experienced Apache Mahout maintainer** who is applying for the mentor role.\n*   The tasks are technically feasible within the GSoC timeframe.\n\n**Conclusion**\n--------------\n\nRefactoring the Apache Mahout website is **essential** for reflecting its **new focus on Quantum Computing (Qumat)** while ensuring historical documentation remains accessible. By modernizing the site, we enhance usability, improve accessibility, and help new users quickly understand Mahout’s direction.\n\nThis project will **significantly enhance Mahout’s online presence** and ensure the community stays well-informed and engaged.\n\n**Difficulty:** Major\n\n**Project size:** ~350 hour (large)\n\n**Potential mentors:**\n\n_Trevor Grant_, mail: rawkintrevo (at) apache.org\n\n_Project Devs_, mail:\n"}